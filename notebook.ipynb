{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c6a8ff",
   "metadata": {},
   "source": [
    "Generating Fraud Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c676990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. URGENT: CEO requests immediate payment to new vendor.\n",
      "2. Unusual activity detected in your Bank of America account. Confirm your identity.\n",
      "3. Official notice: You are the lucky winner of a $500 gift card.\n",
      "4. Congratulations! You've won a Amazon voucher. Claim your reward now.\n",
      "5. Official notice: You are the lucky winner of a $1000 cash prize.\n",
      "6. Medical insurance requires immediate verification.\n",
      "7. Official notice: You are the lucky winner of a $500 gift card.\n",
      "8. Important: Update your Netflix credentials today.\n",
      "9. Your Wells Fargo online banking access is locked. Reset now.\n",
      "10. Medical insurance requires immediate verification.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define themes and templates\n",
    "bank_templates = [\n",
    "    \"Your {bank} account has been suspended. Please verify immediately.\",\n",
    "    \"Unusual activity detected in your {bank} account. Confirm your identity.\",\n",
    "    \"{bank} Security Alert: Immediate verification needed.\",\n",
    "    \"Your {bank} online banking access is locked. Reset now.\",\n",
    "]\n",
    "\n",
    "package_templates = [\n",
    "    \"{carrier} attempted delivery. Pay small fee to reschedule.\",\n",
    "    \"{carrier} parcel undeliverable. Update address to receive package.\",\n",
    "    \"Delivery failed. {carrier} needs confirmation of your details.\",\n",
    "]\n",
    "\n",
    "prize_templates = [\n",
    "    \"Congratulations! You've won a {prize}. Claim your reward now.\",\n",
    "    \"{prize} awaits you! Confirm your winning entry.\",\n",
    "    \"Official notice: You are the lucky winner of a {prize}.\",\n",
    "]\n",
    "\n",
    "password_templates = [\n",
    "    \"Reset your {service} password now due to suspicious login.\",\n",
    "    \"{service} account security compromised. Change password immediately.\",\n",
    "    \"Important: Update your {service} credentials today.\",\n",
    "]\n",
    "\n",
    "ceo_templates = [\n",
    "    \"URGENT: CEO requests immediate payment to new vendor.\",\n",
    "    \"Confidential: Process wire transfer as instructed by CFO.\",\n",
    "    \"Emergency: Transfer funds to supplier before end of day.\",\n",
    "]\n",
    "\n",
    "service_templates = [\n",
    "    \"{service} subscription expired. Update billing information.\",\n",
    "    \"Your {service} account will be terminated. Verify payment.\",\n",
    "    \"{service} service interrupted. Reactivate now.\",\n",
    "]\n",
    "\n",
    "health_templates = [\n",
    "    \"Healthcare update: Confirm enrollment today.\",\n",
    "    \"Urgent: Final notice for health benefits renewal.\",\n",
    "    \"Medical insurance requires immediate verification.\",\n",
    "]\n",
    "\n",
    "# Value pools\n",
    "banks = [\"Bank of America\", \"Chase\", \"Wells Fargo\", \"Citibank\", \"HSBC\"]\n",
    "carriers = [\"FedEx\", \"UPS\", \"DHL\", \"USPS\"]\n",
    "prizes = [\"$500 gift card\", \"$1000 cash prize\", \"new iPhone\", \"Amazon voucher\"]\n",
    "services = [\"Netflix\", \"PayPal\", \"Amazon\", \"Apple\", \"Microsoft\"]\n",
    "\n",
    "# Combine all templates\n",
    "templates = (\n",
    "    bank_templates +\n",
    "    package_templates +\n",
    "    prize_templates +\n",
    "    password_templates +\n",
    "    ceo_templates +\n",
    "    service_templates +\n",
    "    health_templates\n",
    ")\n",
    "\n",
    "# Generate 500 spam messages\n",
    "spam_messages = []\n",
    "\n",
    "for _ in range(500):\n",
    "    template = random.choice(templates)\n",
    "    filled = template.format(\n",
    "        bank=random.choice(banks),\n",
    "        carrier=random.choice(carriers),\n",
    "        prize=random.choice(prizes),\n",
    "        service=random.choice(services)\n",
    "    )\n",
    "    spam_messages.append(filled)\n",
    "\n",
    "# Example output\n",
    "for i in range(10):\n",
    "    print(f\"{i+1}. {spam_messages[i]}\")\n",
    "\n",
    "# Save to text file (optional)\n",
    "with open(\"generated_spam_messages.txt\", \"w\") as f:\n",
    "    for msg in spam_messages:\n",
    "        f.write(msg + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebc962a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    4825\n",
      "1    1139\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load original dataset\n",
    "df_original = pd.read_csv(\"dataset/SMSSpamCollection\", sep='\\t', names=['label', 'text'])\n",
    "\n",
    "\n",
    "# Load generated spam messages\n",
    "with open(\"dataset/generated_spam_messages.txt\", \"r\") as f:\n",
    "    generated_spam = f.readlines()\n",
    "\n",
    "# Clean newlines\n",
    "generated_spam = [msg.strip() for msg in generated_spam]\n",
    "\n",
    "# Create DataFrame\n",
    "df_generated = pd.DataFrame({\n",
    "    'label': ['spam'] * len(generated_spam),\n",
    "    'text': generated_spam\n",
    "})\n",
    "\n",
    "# Combine original + generated\n",
    "df_full = pd.concat([df_original, df_generated], ignore_index=True)\n",
    "\n",
    "# Shuffle to mix generated + original\n",
    "df_full = df_full.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df_full[\"label\"] = df_full[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "print(df_full['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac632a5",
   "metadata": {},
   "source": [
    "Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d22448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "df_full['text'] = df_full['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a13af3",
   "metadata": {},
   "source": [
    "Split to traint Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c0d09e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nohrer/.pyenv/versions/3.11.9/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df_full[\"text\"])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df_full[\"text\"])\n",
    "padded = pad_sequences(sequences, maxlen=50)\n",
    "\n",
    "joblib.dump(tokenizer, \"tokenizer.pkl\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded, df_full[\"label\"].values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 6: Handle class imbalance\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Step 7: Build model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=50),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a54d9",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f355b0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7965 - auc: 0.6502 - loss: 0.6745 - val_accuracy: 0.9765 - val_auc: 0.9894 - val_loss: 0.6162\n",
      "Epoch 2/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9283 - auc: 0.9498 - loss: 0.5374 - val_accuracy: 0.9774 - val_auc: 0.9901 - val_loss: 0.2973\n",
      "Epoch 3/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9791 - auc: 0.9879 - loss: 0.2483 - val_accuracy: 0.9824 - val_auc: 0.9916 - val_loss: 0.1356\n",
      "Epoch 4/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9869 - auc: 0.9920 - loss: 0.1297 - val_accuracy: 0.9673 - val_auc: 0.9922 - val_loss: 0.1661\n",
      "Epoch 5/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9829 - auc: 0.9944 - loss: 0.0994 - val_accuracy: 0.9849 - val_auc: 0.9946 - val_loss: 0.0644\n",
      "Epoch 6/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9930 - auc: 0.9985 - loss: 0.0555 - val_accuracy: 0.9841 - val_auc: 0.9945 - val_loss: 0.0703\n",
      "Epoch 7/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9935 - auc: 0.9990 - loss: 0.0440 - val_accuracy: 0.9807 - val_auc: 0.9946 - val_loss: 0.0711\n",
      "Epoch 8/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9922 - auc: 0.9994 - loss: 0.0382 - val_accuracy: 0.9883 - val_auc: 0.9957 - val_loss: 0.0469\n",
      "Epoch 9/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9960 - auc: 0.9996 - loss: 0.0279 - val_accuracy: 0.9874 - val_auc: 0.9949 - val_loss: 0.0424\n",
      "Epoch 10/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9959 - auc: 0.9998 - loss: 0.0257 - val_accuracy: 0.9858 - val_auc: 0.9942 - val_loss: 0.0494\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight=class_weights_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e914a",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7cd853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"smishing_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f967b9b",
   "metadata": {},
   "source": [
    "model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a928889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - accuracy: 0.8143 - auc: 0.6913 - loss: 0.5599\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m loss, accuracy = model.evaluate(X_test_pad, y_test)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5368557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.85      0.95      0.90       985\n",
      "        spam       0.47      0.19      0.27       208\n",
      "\n",
      "    accuracy                           0.82      1193\n",
      "   macro avg       0.66      0.57      0.58      1193\n",
      "weighted avg       0.78      0.82      0.79      1193\n",
      "\n",
      "[[939  46]\n",
      " [168  40]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Predict\n",
    "y_pred_probs = model.predict(X_test_pad)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aeec7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
